# Attention Is All You Need â€” PyTorch Reproduction (ENâ†”ES MT)

A compact, reproduction of the original **Transformer** architecture in PyTorch, tailored for **Englishâ†”Spanish machine translation** using the OPUSâ€‘100 dataset and **SentencePiece** BPE. This repository emphasizes clarity, correctness, and testabilityâ€”aimed at students and researchers who want a clean, minimal, and modern baseline that trains on a single T4 GPU.

---

## âœ¨ Highlights

- **Faithful encoderâ€“decoder** with **post-norm** sublayers (LayerNorm after residual).
- **Sinusoidal positional encoding** (`register_buffer`), deterministic and unit-tested.
- **Clean dataloading** for OPUS-100 (ENâ€“ES) + **SentencePiece** BPE (shared vocab).
- **Robust masks**: padding + causal look-ahead, broadcast-safe to `(B, H, Lq, Lk)`.
- **Modern training**: Adam(Î²1=0.9, Î²2=0.98, eps=1e-9), **Noam LR** (warmup in *steps*), label smoothing, grad clipping.
- **In-loop metrics (no decoding)**: token accuracy and perplexity (PPL = `exp(loss)`).
- **Teacher-forced preview**: quick *argmax* snapshots to sniff quality during training.
- **Optional weight tying**: `tgt_embed â†” softmax` (+ optional `srcâ†”tgt` if shared vocab) with **correct logits scaling**.
- **Unit tests** (pytest) for PE, masks, MHA, Encoder/Decoder, a full train step, and logits shapes.

---

## ğŸ§± Architecture

- **Token embeddings** (learned end-to-end) + **sinusoidal PosEnc**.  
  *Note:* only the rows (tokens) used in a batch receive gradients/updates.
- **Multi-Head Attention** (scaled dot-product) with trainable **Wq, Wk, Wv, Wo**.
- **Position-wise FFN**: `Linear(d_model â†’ d_ff)` â†’ ReLU â†’ `Linear(d_ff â†’ d_model)` (default `d_ff = 4 Ã— d_model`).
- **Residual + LayerNorm** (**post-norm**) in each sub-block.
- **Final linear to vocab** (no softmax in `forward`; use `CrossEntropyLoss` on logits for numerical stability).

> We intentionally **do not** apply softmax in the model forward; use `CrossEntropyLoss` on logits for numerical stability.

---

## ğŸ“¦ Dataset & Tokenization

- **Dataset**: [`Helsinki-NLP/opus-100`](https://huggingface.co/datasets/Helsinki-NLP/opus-100), language pair **enâ€“es**.
- **SentencePiece BPE** with **shared vocabulary** (default `vocab_size=16000`).
  - Special tokens: `pad_id=0`, `bos_id=1`, `eos_id=2`, `unk_id=3`.
- **Encoding:** `BOS + tokens + EOS`, `max_len` (e.g., `128` or `256`).
- **Collate & masks:**
  - **Key padding mask** `(B, Lk)` â†’ expanded to `(B, 1, 1, Lk)`.
  - **Decoder look-ahead** (triangular causal) possibly combined with tgt padding â†’ `(B, 1, Lq, Lk)`.
  - All masks **broadcastable** to `(B, H, Lq, Lk)`.

---

## âš™ï¸ Default Configuration (Tiny)

```python
d_model   = 256      # embedding width (and model width)
num_layers= 4        # encoder and decoder layers
num_heads = 4        # attention heads (d_head = d_model // num_heads)
d_ff     = 1024      # FFN hidden size (â‰ˆ 4 Ã— d_model)
vocab    = 16000     # SentencePiece shared vocab
max_pos  = 256       # max positions for PE (>= max sequence length)
```

This configuration fits **comfortably on a T4** with a small subset of OPUSâ€‘100 (e.g., 50k sentence pairs).


### Training

- **Scheduler:** Noam (Vaswani et al. 2017)

$$
lr(\text{step}) = d_{\text{model}}^{0.5} \min(\text{step}^{-0.5},\; \text{step} \cdot \text{warmup}^{-1.5})
$$

`warmup` is in steps (batches), not epochs.

- **Recommended for 50k pairs, batch=64:**
  - steps_per_epoch â‰ˆ 50,000 / 64 â‰ˆ 782
  - warmup â‰ˆ 1,600 (â‰ˆ 2 Ã— steps/epoch)
  - log_every â‰ˆ 80 (â‰ˆ 10 logs/epoch)
  - epochs = 10â€“20

- **Other:** gradient clipping (e.g., 1.0), AMP (mixed precision) recommended on T4.

### In-loop preview (teacher forcing):
Optionally print `argmax` predictions vs reference to monitor progress without decoding:

```python
HYP: ...  # model argmax under teacher forcing
REF: ...
```

## ğŸ“ Transformer Math & Complexity (Quick)

### Scaled Dot-Product Attention
Given $Q\in\mathbb{R}^{L_q\times d_k}$, $K\in\mathbb{R}^{L_k\times d_k}$, $V\in\mathbb{R}^{L_k\times d_v}$:

$$\mathrm{Attn}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}+M\right)V$$

Time (single head): $O(L_qL_k d_k)$; Memory: $O(L_qL_k)$ for attention weights.

### Multi-Head Attention (MHA)

$$\mathrm{MHA}(X)=\mathrm{Concat}(H_1,\dots,H_h)W_O,\quad H_i=\mathrm{Attn}(XW_i^Q,XW_i^K,XW_i^V)$$

With $d_h=d/h$: Params $\approx 4d^2$ (independent of $h$).  

Self-attn time ( $L_q=L_k=L$ ): $O(L^2 d + L d^2)$; Memory: $O(L^2)$.

### Position-wise FFN

$$\mathrm{FFN}(x)=\max(0,\,xW_1+b_1)W_2+b_2$$

Params $\approx 2 d\,d_{ff}$; Time $O(L\,d\,d_{ff})$; Activations $O(L\,d_{ff})$.

### Encoder Layer (post-norm)
1) $X\leftarrow \mathrm{LN}(X+\mathrm{MHA}(X))$  
2) $X\leftarrow \mathrm{LN}(X+\mathrm{FFN}(X))$

Per-layer time: $O(L^2 d + L d^2 + L d d_{ff})$ (bottleneck: $L^2 d$).

### Decoder Layer (post-norm)
Masked self-attn on $T$: $O(T^2 d)$; cross-attn to encoder length $S$: $O(T S d)$; FFN: $O(T d d_{ff})$.

### Masks (convention)
- **Padding:** boolean $True$ = block `<pad>`; broadcast to $(B,1,L_q,L_k)$.  
- **Look-ahead:** upper-triangular $[T\times T]$, $True$ = block future.  
- **Decoder self-attn:** combine as $M_{\text{look}} \lor M_{\text{pad-tgt}}$.

**Tips:** To fit on small GPUs, reduce $L$, $d$, or $d_{ff}$, or batch size. Use Noam schedule and label smoothing for stability.




---

## ğŸ§ª Tests (pytestâ€‘style)

The repository includes **selfâ€‘contained, assertâ€‘based tests** grouped by module:

```
tests/
  conftest.py
  test_positional_encoding.py
  test_masks.py
  test_mha.py
  test_encoder_decoder.py
  test_train_step.py
```

**What we test:**
- Positional encoding shapes/values and numerics.
- Mask creation and combination (padding + lookâ€‘ahead).
- MHA shape contracts and gradient flow.
- Encoder/Decoder forward passes (finite outputs).
- A full training step: forward â†’ loss â†’ backward â†’ optimizer step.

Run all tests:
```bash 
pytest -q
```

> If you donâ€™t use pytest, you can still run the test functions directly; they are standard Python `assert` checks.

---

## ğŸ“ Project Structure (suggested)

```
attention-is-all-you-need/
â”‚
â”œâ”€ data/                            # Data loading & preprocessing
â”‚   â”œâ”€ load_data.py                 # Dataset loading (HuggingFace / OPUS-100)
â”‚   â””â”€ evaluate_data.py             # Corpus sanity checks, length stats, etc.
â”‚
â”œâ”€ model/
â”‚   â”œâ”€ architecture/                # Core Transformer components
â”‚   â”‚   â”œâ”€ __init__.py              # Reexports for cleaner imports
â”‚   â”‚   â”œâ”€ attention.py             # Multi-Head Attention (Q, K, V, scaled dot-product)
â”‚   â”‚   â”œâ”€ encoder.py               # Encoder block (Self-Attention + FFN)
â”‚   â”‚   â”œâ”€ decoder.py               # Decoder block (Masked MHA + Cross-Attention)
â”‚   â”‚   â”œâ”€ pos_encoding.py          # Positional Encoding (sinusoidal)
â”‚   â”‚   â””â”€ transformer_masks.py     # Padding & look-ahead masks
â”‚   â”‚
â”‚   â”œâ”€ training/                    # Training utilities and loop
â”‚   â”‚   â”œâ”€ training_loop.py         # Full training step (forward + backward)
â”‚   â”‚   â”œâ”€ training_utils.py        # LR schedulers, label smoothing, clipping
â”‚   â”‚   â””â”€ transformer_utils.py     # Weight initialization, parameter counting
â”‚   â”‚
â”‚   â”œâ”€ transformer.py               # High-level Encoderâ€“Decoder assembly
â”‚
â”œâ”€ test/                            # Unit tests (pytest style)
â”‚   â”œâ”€ conftest.py                  # Pytest fixtures (mocks, sample tensors)
â”‚   â”œâ”€ test_mha.py                  # Tests for Multi-Head Attention
â”‚   â”œâ”€ test_positional_encoding.py  # PE correctness and shape
â”‚   â”œâ”€ test_masks.py                # Mask logic (padding & look-ahead)
â”‚   â”œâ”€ test_encoder_decoder.py      # Encoder/Decoder forward consistency
â”‚   â””â”€ test_train_step.py           # Gradient flow, optimizer updates
â”‚
â”œâ”€ training_showcase/               # Example notebooks
â”‚   â””â”€ training_showcase.ipynb      # Interactive training + evaluation demo
â”‚
â”œâ”€ README.md                        # Project overview, setup, usage
```

---

## âœ… Tips & Notes

- If you see early training instability, increase `warmup` (e.g., 6000â€“8000).
- To reduce memory: lower `batch_size`, `d_model`, or `num_layers`.
- Keep `d_model` divisible by `num_heads`.
- Do **not** apply softmax in the model; use CE on logits.
- Label smoothing (0.1) can help generalization on small corpora.

---

## ğŸ“š References

- Vaswani et al., *Attention Is All You Need*, 2017.  
- OPUSâ€‘100: A Multi-Lingual Dataset for Machine Translation.  
- SentencePiece: *A simple and language independent subword tokenizer and detokenizer*.

---

## ğŸ“œ License

MIT License â€” see `LICENSE` for details.
